{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tiny-vgg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1NBLBJqTsby_q6XdGLT-GMVc1vRR6mo7d",
      "authorship_tag": "ABX9TyMYF67Mwlppm4phGpoirYHC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prodramp/python-projects/blob/main/deeplearning/cnn-explainer/Tiny_vgg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gfo_EDKSCoFC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from shutil import copyfile\n",
        "from glob import glob\n",
        "from json import load, dump\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D,\\\n",
        "    Activation\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from os.path import basename\n",
        "from time import time\n",
        "\n",
        "print(tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBgJeEdYCrhv",
        "outputId": "c2baa40d-a848-413b-8e5d-cd002bb43d21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_class_dict():\n",
        "    # Create a new version only including tiny 200 classes\n",
        "    df = pd.read_csv('./tiny-imagenet-200/words.txt', sep='\\t', header=None)\n",
        "    keys, classes = df[0], df[1]\n",
        "    class_dict = dict(zip(keys, classes))\n",
        "\n",
        "    tiny_class_dict = {}\n",
        "    cur_index = 0\n",
        "\n",
        "    for directory in glob('./tiny-imagenet-200/train/*'):\n",
        "        cur_key = basename(directory)\n",
        "        tiny_class_dict[cur_key] = {'class': class_dict[cur_key],\n",
        "                                    'index': cur_index}\n",
        "        cur_index += 1\n",
        "\n",
        "    dump(tiny_class_dict, open('./tiny-imagenet-200/class_dict.json', 'w'),\n",
        "         indent=2)\n",
        "\n",
        "\n",
        "def create_val_class_dict():\n",
        "    tiny_class_dict = load(open('./tiny-imagenet-200/class_dict.json', 'r'))\n",
        "    tiny_val_class_dict = {}\n",
        "\n",
        "    # Create a dictionary for validation images\n",
        "    df = pd.read_csv('./tiny-imagenet-200/val/val_annotations.txt', sep='\\t',\n",
        "                     header=None)\n",
        "    image_names = df[0]\n",
        "    image_classes = df[1]\n",
        "\n",
        "    for i in range(len(image_names)):\n",
        "        tiny_val_class_dict[image_names[i]] = {\n",
        "            'class': tiny_class_dict[image_classes[i]]['class'],\n",
        "            'index': tiny_class_dict[image_classes[i]]['index'],\n",
        "        }\n",
        "\n",
        "    dump(tiny_val_class_dict, open('./tiny-imagenet-200/val_class_dict.json',\n",
        "                                   'w'),\n",
        "         indent=2)\n",
        "\n",
        "\n",
        "def split_val_data():\n",
        "    # Split validation images to 50% early stopping and 50% hold-out testing\n",
        "    val_images = glob('./tiny-imagenet-200/val/images/*.JPEG')\n",
        "    np.random.shuffle(val_images)\n",
        "\n",
        "    for i in range(len(val_images)):\n",
        "        if i < len(val_images) // 2:\n",
        "            copyfile(val_images[i], val_images[i].replace('images',\n",
        "                                                          'val_images'))\n",
        "        else:\n",
        "            copyfile(val_images[i], val_images[i].replace('images',\n",
        "                                                          'test_images'))\n",
        "\n",
        "\n",
        "def process_path_train(path):\n",
        "    \"\"\"\n",
        "    Get the (class label, processed image) pair of the given image path. This\n",
        "    funciton uses python primitives, so you need to use tf.py_funciton wrapper.\n",
        "    This function uses global variables:\n",
        "\n",
        "        WIDTH(int): the width of the targeting image\n",
        "        HEIGHT(int): the height of the targeting iamge\n",
        "        NUM_CLASS(int): number of classes\n",
        "\n",
        "    Args:\n",
        "        path(string): path to an image file\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the class\n",
        "    path = path.numpy()\n",
        "    image_name = basename(path.decode('ascii'))\n",
        "    label_name = re.sub(r'(.+)_\\d+\\.JPEG', r'\\1', image_name)\n",
        "    label_index = tiny_class_dict[label_name]['index']\n",
        "\n",
        "    # Convert label to one-hot encoding\n",
        "    label = tf.one_hot(indices=[label_index], depth=NUM_CLASS)\n",
        "    label = tf.reshape(label, [NUM_CLASS])\n",
        "\n",
        "    # Read image and convert the image to [0, 1] range 3d tensor\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    img = tf.image.resize(img, [WIDTH, HEIGHT])\n",
        "\n",
        "    return(img, label)\n",
        "\n",
        "\n",
        "def process_path_test(path):\n",
        "    \"\"\"\n",
        "    Get the (class label, processed image) pair of the given image path. This\n",
        "    funciton uses python primitives, so you need to use tf.py_funciton wrapper.\n",
        "    This function uses global variables:\n",
        "\n",
        "        WIDTH(int): the width of the targeting image\n",
        "        HEIGHT(int): the height of the targeting iamge\n",
        "        NUM_CLASS(int): number of classes\n",
        "\n",
        "    The filepath encoding for test images is different from training images.\n",
        "\n",
        "    Args:\n",
        "        path(string): path to an image file\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the class\n",
        "    path = path.numpy()\n",
        "    image_name = basename(path.decode('ascii'))\n",
        "    label_index = tiny_val_class_dict[image_name]['index']\n",
        "\n",
        "    # Convert label to one-hot encoding\n",
        "    label = tf.one_hot(indices=[label_index], depth=NUM_CLASS)\n",
        "    label = tf.reshape(label, [NUM_CLASS])\n",
        "\n",
        "    # Read image and convert the image to [0, 1] range 3d tensor\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    img = tf.image.resize(img, [WIDTH, HEIGHT])\n",
        "\n",
        "    return(img, label)\n",
        "\n",
        "\n",
        "def prepare_for_training(dataset, batch_size=32, cache=True,\n",
        "                         shuffle_buffer_size=1000):\n",
        "\n",
        "    if cache:\n",
        "        if isinstance(cache, str):\n",
        "            dataset = dataset.cache(cache)\n",
        "        else:\n",
        "            dataset = dataset.cache()\n",
        "\n",
        "    # Only shuffle elements in the buffer size\n",
        "    dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "    # Pre featch batches in the background\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def prepare_for_testing(dataset, batch_size=32, cache=True):\n",
        "    if cache:\n",
        "        if isinstance(cache, str):\n",
        "            dataset = dataset.cache(cache)\n",
        "        else:\n",
        "            dataset = dataset.cache()\n",
        "\n",
        "    # Pre featch batches in the background\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "class TinyVGG(Model):\n",
        "    \"\"\"\n",
        "    Tiny VGG structure is adapted from http://cs231n.stanford.edu:\n",
        "        > This particular network is classifying CIFAR-10 images into one of 10\n",
        "        > classes and was trained with ConvNetJS. Its exact architecture is\n",
        "        > [conv-relu-conv-relu-pool]x3-fc-softmax, for a total of 17 layers and\n",
        "        > 7000 parameters. It uses 3x3 convolutions and 2x2 pooling regions.\n",
        "    \"\"\"\n",
        "    def __init__(self, filters=10):\n",
        "        super(TinyVGG, self).__init__()\n",
        "        self.conv_1_1 = Conv2D(filters, (3, 3), name='conv_1_1')\n",
        "        self.relu_1_1 = Activation('relu', name='relu_1_1')\n",
        "        self.conv_1_2 = Conv2D(filters, (3, 3), name='conv_1_2')\n",
        "        self.relu_1_2 = Activation('relu', name='relu_1_2')\n",
        "        self.max_pool_1 = MaxPool2D((2, 2), name='max_pool_1')\n",
        "\n",
        "        self.conv_2_1 = Conv2D(filters, (3, 3), name='conv_2_1')\n",
        "        self.relu_2_1 = Activation('relu', name='relu_2_1')\n",
        "        self.conv_2_2 = Conv2D(filters, (3, 3), name='conv_2_2')\n",
        "        self.relu_2_2 = Activation('relu', name='relu_2_2')\n",
        "        self.max_pool_2 = MaxPool2D((2, 2), name='max_pool_2')\n",
        "\n",
        "        self.flatten = Flatten()\n",
        "        self.fc = Dense(NUM_CLASS, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv_1_1(x)\n",
        "        x = self.relu_1_1(x)\n",
        "        x = self.conv_1_2(x)\n",
        "        x = self.relu_1_2(x)\n",
        "        x = self.max_pool_1(x)\n",
        "\n",
        "        x = self.conv_2_1(x)\n",
        "        x = self.relu_2_1(x)\n",
        "        x = self.conv_2_2(x)\n",
        "        x = self.relu_2_2(x)\n",
        "        x = self.max_pool_2(x)\n",
        "\n",
        "        x = self.conv_3_1(x)\n",
        "        x = self.relu_3_1(x)\n",
        "        x = self.conv_3_2(x)\n",
        "        x = self.relu_3_2(x)\n",
        "        x = self.max_pool_3(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(image_batch, label_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Predict\n",
        "        predictions = tiny_vgg(image_batch)\n",
        "\n",
        "        # Update gradient\n",
        "        loss = loss_object(label_batch, predictions)\n",
        "        gradients = tape.gradient(loss, tiny_vgg.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, tiny_vgg.trainable_variables))\n",
        "\n",
        "        train_mean_loss(loss)\n",
        "        train_accuracy(label_batch, predictions)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def vali_step(image_batch, label_batch):\n",
        "    predictions = tiny_vgg(image_batch)\n",
        "    vali_loss = loss_object(label_batch, predictions)\n",
        "\n",
        "    vali_mean_loss(vali_loss)\n",
        "    vali_accuracy(label_batch, predictions)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def test_step(image_batch, label_batch):\n",
        "    predictions = tiny_vgg(image_batch)\n",
        "    test_loss = loss_object(label_batch, predictions)\n",
        "\n",
        "    test_mean_loss(test_loss)\n",
        "    test_accuracy(label_batch, predictions)"
      ],
      "metadata": {
        "id": "-F_bYdwJCv_g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WIDTH = 64\n",
        "HEIGHT = 64\n",
        "EPOCHS = 1000\n",
        "PATIENCE = 50\n",
        "LR = 0.001\n",
        "NUM_CLASS = 10\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "C7I5MHOBGFfr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation dataset\n",
        "tiny_class_dict = load(open('class_dict_10.json', 'r'))\n",
        "tiny_val_class_dict = load(open('val_class_dict_10.json', 'r'))"
      ],
      "metadata": {
        "id": "4NWauwIzGFme"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah ../content/drive/MyDrive/devdata/tiny-vgg/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV07TtIILarX",
        "outputId": "e55c8736-da3d-496e-b4ff-66a69486b9cb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 67K\n",
            "drwx------ 12 root root 4.0K Apr  8 21:26 class_10_train\n",
            "drwx------  4 root root 4.0K Apr  8 21:56 class_10_val\n",
            "-rw-------  1 root root  817 Nov  8  2019 class_dict_10.json\n",
            "-rw-------  1 root root  58K Nov  8  2019 val_class_dict_10.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_images = '../content/drive/MyDrive/devdata/tiny-vgg/data/class_10_train/*/images/*.JPEG'\n",
        "vali_images = '../content/drive/MyDrive/devdata/tiny-vgg/data/class_10_val/val_images/*.JPEG'\n",
        "test_images = '../content/drive/MyDrive/devdata/tiny-vgg/data/class_10_val/test_images/*.JPEG'"
      ],
      "metadata": {
        "id": "eUUKWLZ6GM7i"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training dataset\n",
        "train_path_dataset = tf.data.Dataset.list_files(training_images)\n",
        "\n",
        "train_labeld_dataset = train_path_dataset.map(\n",
        "    lambda path: tf.py_function(\n",
        "        process_path_train,\n",
        "        [path],\n",
        "        [tf.float32, tf.float32]\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "YXzhGs6tLufB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create vali dataset\n",
        "vali_path_dataset = tf.data.Dataset.list_files(vali_images)\n",
        "\n",
        "vali_labeld_dataset = vali_path_dataset.map(\n",
        "    lambda path: tf.py_function(\n",
        "        process_path_test,\n",
        "        [path],\n",
        "        [tf.float32, tf.float32]\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "47tzXpy6Lwvf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create test dataset\n",
        "test_path_dataset = tf.data.Dataset.list_files(test_images)\n",
        "\n",
        "test_labeld_dataset = test_path_dataset.map(\n",
        "    lambda path: tf.py_function(\n",
        "        process_path_test,\n",
        "        [path],\n",
        "        [tf.float32, tf.float32]\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "pUWNnmh3LyhT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dataset = prepare_for_training(train_labeld_dataset,\n",
        "                                     batch_size=BATCH_SIZE)\n",
        "vali_dataset = prepare_for_training(vali_labeld_dataset,\n",
        "                                    batch_size=BATCH_SIZE)\n",
        "test_dataset = prepare_for_training(test_labeld_dataset,\n",
        "                                    batch_size=BATCH_SIZE)\n",
        "\n"
      ],
      "metadata": {
        "id": "GIyoP4m8C0fl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55zIStv-L2L9",
        "outputId": "04bffd6b-51e7-4993-b5a2-605807577f68"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=<unknown>, dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the model\n",
        "# tiny_vgg = TinyVGG()\n",
        "\n",
        "# Use Keras Sequential API instead, since it is easy to save the model\n",
        "filters = 10\n",
        "tiny_vgg = Sequential([\n",
        "    Conv2D(filters, (3, 3), input_shape=(64, 64, 3), name='conv_1_1'),\n",
        "    Activation('relu', name='relu_1_1'),\n",
        "    ## 62,62, 10\n",
        "\n",
        "    Conv2D(filters, (3, 3), name='conv_1_2'), ## 60,60, 10\n",
        "    Activation('relu', name='relu_1_2'),\n",
        "    ## 60, 60 >> 30,30\n",
        "    MaxPool2D((2, 2), name='max_pool_1'),\n",
        "\n",
        "    Conv2D(filters, (3, 3), name='conv_2_1'), ## 28,28, 10\n",
        "    Activation('relu', name='relu_2_1'),\n",
        "\n",
        "    Conv2D(filters, (3, 3), name='conv_2_2'), ## 26,26, 10\n",
        "    Activation('relu', name='relu_2_2'), ## \n",
        "    MaxPool2D((2, 2), name='max_pool_2'), ## 13, 13, 10\n",
        "\n",
        "    # Input - 13x13,10 = 13*13*10 = 1690\n",
        "    Flatten(name='flatten'),\n",
        "    Dense(NUM_CLASS, activation='softmax', name='output')\n",
        "])\n",
        "\n",
        "# \"Compile\" the model with loss function and optimizer\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=LR)\n",
        "\n",
        "train_mean_loss = tf.keras.metrics.Mean(name='train_mean_loss')\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "vali_mean_loss = tf.keras.metrics.Mean(name='vali_mean_loss')\n",
        "vali_accuracy = tf.keras.metrics.CategoricalAccuracy(name='vali_accuracy')"
      ],
      "metadata": {
        "id": "Rqr0aXo3L5-_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Train\n",
        "    for image_batch, label_batch in train_dataset:\n",
        "        train_step(image_batch, label_batch)\n",
        "\n",
        "    # Predict on the test dataset\n",
        "    for image_batch, label_batch in vali_dataset:\n",
        "        vali_step(image_batch, label_batch)\n",
        "\n",
        "    template = 'epoch: {}, train loss: {:.4f}, train accuracy: {:.4f}, '\n",
        "    template += 'vali loss: {:.4f}, vali accuracy: {:.4f}'\n",
        "    print(template.format(epoch + 1,\n",
        "                          train_mean_loss.result(),\n",
        "                          train_accuracy.result() * 100,\n",
        "                          vali_mean_loss.result(),\n",
        "                          vali_accuracy.result() * 100))\n",
        "\n",
        "    # Early stopping\n",
        "    if vali_mean_loss.result() < best_vali_loss:\n",
        "        no_improvement_epochs = 0\n",
        "        best_vali_loss = vali_mean_loss.result()\n",
        "        # Save the best model\n",
        "        tiny_vgg.save('trained_vgg_best.h5')\n",
        "    else:\n",
        "        no_improvement_epochs += 1\n",
        "\n",
        "    if no_improvement_epochs >= PATIENCE:\n",
        "        print('Early stopping at epoch = {}'.format(epoch))\n",
        "        break\n",
        "\n",
        "    # Reset evaluation metrics\n",
        "    train_mean_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    vali_mean_loss.reset_states()\n",
        "    vali_accuracy.reset_states()\n",
        "\n",
        "print('\\nFinished training, used {:.4f} mins.'.format((time() -\n",
        "                                                       start_time) / 60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "E3zff7WqGXeU",
        "outputId": "360765f0-5d65-4a1f-d3ab-d827a9c19b62"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 192, train loss: 0.6786, train accuracy: 77.7000, vali loss: 0.7876, vali accuracy: 70.3704\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-096ff9352dc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Predict on the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_vgg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Urx_rT29f_NH",
        "outputId": "3d3e5cfd-ae3b-4920-fb17-32b5c59795e5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f1c0c0aea50>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_vgg.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeblkF2z2d6h",
        "outputId": "863eee12-8e45-4e01-aaae-262a496943e4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv_1_1 (Conv2D)           (None, 62, 62, 10)        280       \n",
            "                                                                 \n",
            " relu_1_1 (Activation)       (None, 62, 62, 10)        0         \n",
            "                                                                 \n",
            " conv_1_2 (Conv2D)           (None, 60, 60, 10)        910       \n",
            "                                                                 \n",
            " relu_1_2 (Activation)       (None, 60, 60, 10)        0         \n",
            "                                                                 \n",
            " max_pool_1 (MaxPooling2D)   (None, 30, 30, 10)        0         \n",
            "                                                                 \n",
            " conv_2_1 (Conv2D)           (None, 28, 28, 10)        910       \n",
            "                                                                 \n",
            " relu_2_1 (Activation)       (None, 28, 28, 10)        0         \n",
            "                                                                 \n",
            " conv_2_2 (Conv2D)           (None, 26, 26, 10)        910       \n",
            "                                                                 \n",
            " relu_2_2 (Activation)       (None, 26, 26, 10)        0         \n",
            "                                                                 \n",
            " max_pool_2 (MaxPooling2D)   (None, 13, 13, 10)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1690)              0         \n",
            "                                                                 \n",
            " output (Dense)              (None, 10)                16910     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,920\n",
            "Trainable params: 19,920\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model\n",
        "tiny_vgg.save('trained_tiny_vgg.h5')\n",
        "tiny_vgg = tf.keras.models.load_model('trained_vgg_best.h5')\n",
        "\n",
        "# Test on hold-out test images\n",
        "test_mean_loss = tf.keras.metrics.Mean(name='test_mean_loss')\n",
        "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "for image_batch, label_batch in test_dataset:\n",
        "    test_step(image_batch, label_batch)\n",
        "\n",
        "template = '\\ntest loss: {:.4f}, test accuracy: {:.4f}'\n",
        "print(template.format(test_mean_loss.result(),\n",
        "                      test_accuracy.result() * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5B2QzO3GbNF",
        "outputId": "8fe1f93a-e8ed-4db9-c26f-a880ae1411e1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "\n",
            "test loss: 1.1316, test accuracy: 66.4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls  -lah ../content/drive/MyDrive/devdata/tiny-vgg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTs_2vevC3yG",
        "outputId": "eb62d055-818f-4b2a-d7d3-7bcf31dc532c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4.0K\n",
            "drwx------ 2 root root 4.0K Apr  8 21:30 data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vuo_ZJgdFnEv",
        "outputId": "d7eee84f-242e-47db-cfef-2c1f09a9f040"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12M\n",
            "drwxr-xr-x 1 root root 4.0K Apr  8 23:26 .\n",
            "drwxr-xr-x 1 root root 4.0K Apr  8 21:22 ..\n",
            "-rw-r--r-- 1 root root  817 Apr  8 21:32 class_dict_10.json\n",
            "drwxr-xr-x 4 root root 4.0K Mar 23 14:21 .config\n",
            "-rw-r--r-- 1 root root  12M Apr  8 21:22 data.zip\n",
            "drwx------ 7 root root 4.0K Apr  8 21:24 drive\n",
            "drwxr-xr-x 1 root root 4.0K Mar 23 14:22 sample_data\n",
            "-rw-r--r-- 1 root root 111K Apr  8 23:26 trained_tiny_vgg.h5\n",
            "-rw-r--r-- 1 root root 111K Apr  8 23:26 trained_vgg_best.h5\n",
            "-rw-r--r-- 1 root root  58K Apr  8 21:32 val_class_dict_10.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KT4fcjfSF-xg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}